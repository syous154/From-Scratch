{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb ## ipynb파일을 import 하게 도와주는 모듈\n",
    "import model\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.ops import stochastic_depth\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "from torchvision.transforms import AutoAugmentPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class decode_and_center_crop(object):\n",
    "    def __init__(self, image_size=224, crop_fraction=0.875):\n",
    "        self.image_size = image_size\n",
    "        self.crop_fraction = crop_fraction\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Get image size\n",
    "        image_width, image_height = img.size\n",
    "\n",
    "        # Calculate crop parameters\n",
    "        crop_padding = round(self.image_size * (1 / self.crop_fraction - 1))\n",
    "        padded_center_crop_size = int((self.image_size / (self.image_size + crop_padding)) * min(image_height, image_width))\n",
    "\n",
    "        # Calculate crop window\n",
    "        offset_height = (image_height - padded_center_crop_size + 1) // 2\n",
    "        offset_width = (image_width - padded_center_crop_size + 1) // 2\n",
    "        crop_window = (offset_width, offset_height, offset_width + padded_center_crop_size, offset_height + padded_center_crop_size)\n",
    "\n",
    "        # Apply crop\n",
    "        img = img.crop(crop_window)\n",
    "\n",
    "        # Apply resize\n",
    "        img = img.resize((self.image_size, self.image_size), Image.BICUBIC)    \n",
    "\n",
    "        return img\n",
    "\n",
    "class decode_and_random_crop:\n",
    "    def __init__(self, image_size=224):\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(self.image_size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.)),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'DATA ADDRESS'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    decode_and_center_crop(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.AutoAugment(policy=AutoAugmentPolicy.CIFAR10),  # AutoAugment 적용\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    decode_and_center_crop(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "# Load the entire training data\n",
    "full_trainset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                             download=True, transform=train_transform)\n",
    "\n",
    "# Split the full training set into a training set and a validation set\n",
    "train_size = 4000\n",
    "val_size = 1000\n",
    "trainset, remaining = torch.utils.data.random_split(full_trainset, [train_size, len(full_trainset) - train_size])\n",
    "valset, _ = torch.utils.data.random_split(remaining, [val_size, len(remaining) - val_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32,\n",
    "                                        shuffle=False, num_workers=0)\n",
    "\n",
    "# Load the entire test data\n",
    "full_testset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                            download=True, transform=test_transform)\n",
    "\n",
    "# Use only the first 10000 test data\n",
    "test_size = 1000\n",
    "_, testset = torch.utils.data.random_split(full_testset, [len(full_testset) - test_size, test_size])\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "def train_model(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        # 스케줄러 업데이트\n",
    "        #flag = False\n",
    "        #if epoch > 0 and epoch % 2 == 0 and flag == True:\n",
    "        #  optimizer.param_groups[0]['lr'] *= 0.97\n",
    "        #  flag = False\n",
    "        #if epoch > 0 and epoch % 3 == 0 and flag == False:\n",
    "        #  optimizer.param_groups[0]['lr'] *= 0.97\n",
    "        #  flag = True\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        #train_losses.append(running_loss / len(trainloader))\n",
    "        #train_accuracies.append(accuracy)\n",
    "        \n",
    "        print('[%d/%d] loss: %.3f, accuracy: %.2f %%' % (epoch + 1, epochs, running_loss / len(trainloader), accuracy))\n",
    "        \n",
    "        # 검증 세트에서 손실과 정확도 계산\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        #val_losses.append(val_running_loss / len(valloader))\n",
    "        #val_accuracies.append(val_accuracy)\n",
    "        print('[%d/%d] val loss: %.3f, val accuracy: %.2f %%' % (epoch + 1, epochs, val_running_loss / len(valloader), val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 정확도 확인\n",
    "def test_model(model):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for data in valloader:\n",
    "          images, labels = data[0].to(device), data[1].to(device)\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  #print('Accuracy on the test images: %.2f %%' % (100 * correct / total))\n",
    "  return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 가능한 값의 범위를 정의합니다.\n",
    "min_value = 1\n",
    "max_value = 2\n",
    "alphas = []\n",
    "betas = []\n",
    "gammas = []\n",
    "# 가능한 모든 조합을 반복합니다.\n",
    "for alpha in np.arange(min_value, max_value, 0.05):\n",
    "    for beta in np.arange(min_value, max_value, 0.05):\n",
    "        for gamma in np.arange(min_value, max_value, 0.05):\n",
    "            # 식을 계산합니다.\n",
    "            value = alpha * (beta ** 2) * (gamma ** 2)\n",
    "            # 식이 2에 가까운지 확인합니다.\n",
    "            if np.isclose(value, 2, atol=0.1):\n",
    "                alpha = round(alpha, 2)\n",
    "                beta = round(beta, 2)\n",
    "                gamma = round(gamma, 2)\n",
    "\n",
    "                alphas.append(alpha)\n",
    "                betas.append(beta)\n",
    "                gammas.append(gamma)\n",
    "                print(f\"알파: {alpha}, 베타: {beta}, 감마: {gamma}, value = {value}\")\n",
    "\n",
    "                \n",
    "            if alpha == 1.2 and beta == 1.1 and gamma == 1.15:\n",
    "                \n",
    "                print(f\"알파: {alpha}, 베타: {beta}, 감마: {gamma}, value = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc = [] ## 파라미터 별 val acc를 저장\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for i in range(len(alphas)):\n",
    "    model = model.EfficientNetB0(num_classes = 10,width_coef = betas[i], depth_coef = alphas[i], resolution_coef = gammas[i]).to(device)\n",
    "\n",
    "    train_model(model = model, epochs = 10)\n",
    "  \n",
    "    models_acc.append(test_model(model = model))\n",
    "    del model\n",
    "    torch.cuda.empty_cache()  # GPU 메모리 해제 시도\n",
    "    \n",
    "    print(f'{i+1}회 완료')\n",
    "    \n",
    "max_acc = max(models_acc)\n",
    "max_index = models_acc.index(max_acc)\n",
    "alpha = alphas[max_index]\n",
    "beta = betas[max_index]\n",
    "gamma = gammas[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'가장 높은 acc를 보이는 파라미터의 조합 alpha : {alpha}, beta : {beta}, gamma : {gamma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phis = [0, 1, 2, 3, 4, 5, 6, 7]  #b0 ~ b7까지의 파이 값\n",
    "a = 1.2\n",
    "b = 1.1\n",
    "c = 1.15\n",
    "index = 0\n",
    "for phi in phis:\n",
    "  print(f'efficientnet-b{index} : ({round(b**(phi),1)}, {round(a**(phi),1)}, {round(c**(phi)*224,0)})')\n",
    "  index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(케라스 EfficientNet 문서 참고(https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/)) \n",
    "phis = [0, 0.5, 1, 2, 3.8, 5.15, 6.2, 7.2]  #파이 값 수정\n",
    "a = 1.17                                    #알파 값 수정 \n",
    "b = 1.1\n",
    "c = 1.15\n",
    "index = 0\n",
    "for phi in phis:\n",
    "  print(f'efficientnet-b{index} : ({round(b**(phi),1)}, {round(a**(phi),1)}, {round(round(c**(phi),2)*224,0)})')\n",
    "  index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
