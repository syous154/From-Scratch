{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.ops import stochastic_depth\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "from torchvision.transforms import AutoAugmentPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_repeats(repeats, depth_coefficient):\n",
    "    return int(math.ceil(depth_coefficient * repeats))\n",
    "\n",
    "def round_filters(filters, width_coefficient, depth_divisor):\n",
    "    filters *= width_coefficient\n",
    "    new_filters = int(filters + depth_divisor / 2) // depth_divisor * depth_divisor\n",
    "    new_filters = max(depth_divisor, new_filters)\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += depth_divisor\n",
    "    return int(new_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channel, r, expand):                                                                        ### 아래 EfficientNet의 SE 과정을 보면 r=0.04인것 처럼 진행함(논문에선 0.25이것도 모름)\n",
    "        super().__init__()\n",
    "        \n",
    "        ## 축소될 채널 수 계산\n",
    "        if expand == 1:\n",
    "          r = 0.25\n",
    "        sq_channel = max(1,int(in_channel * r))\n",
    "            \n",
    "        \n",
    "        self.se = nn.Sequential(\n",
    "            ## squeeze\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            \n",
    "            ## Exitation\n",
    "            nn.Conv2d(in_channel, sq_channel, kernel_size = 1),\n",
    "            nn.SiLU(inplace = True),\n",
    "            nn.Conv2d(sq_channel, in_channel,kernel_size = 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "    \n",
    "# Chaeck용\n",
    "if __name__ == '__main__':\n",
    "    x = torch.randn(3, 3, 224, 224)\n",
    "    model = SEBlock(x.size(1), 0.04, 1)\n",
    "    output = model(x)\n",
    "    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, expand, padding_size ,stride=1 ,r=0.04, dropout_rate=0.2, p = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 변수 설정\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.expand = expand\n",
    "        self.p = p\n",
    "        # skip connection 사용을 위한 조건 지정\n",
    "        self.use_residual = in_channels == out_channels and stride == 1        #stride가 1이면서 input 채널과 ouput 채널의 수가 같으면 use_residual의 값은 True\n",
    "        \n",
    "        expand_channels = in_channels * expand                                  #확장된 채널 수 계산\n",
    "\n",
    "        ## Expand Phase\n",
    "        self.expasion = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, expand_channels,kernel_size= 1, stride = 1),\n",
    "            nn.BatchNorm2d(expand_channels, momentum=0.99),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        ## Depthwise Conv Phase\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(expand_channels, expand_channels, kernel_size= kernel_size, \n",
    "                      stride=stride, padding = padding_size, groups = expand_channels),\n",
    "            nn.BatchNorm2d(expand_channels, momentum=0.99),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        ## SE Phase\n",
    "        self.seblock = SEBlock(expand_channels, r, expand)\n",
    "        ## Pointwise Conv Phase\n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv2d(expand_channels, out_channels,kernel_size= 1, stride = 1),\n",
    "            nn.BatchNorm2d(out_channels, momentum=0.99)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.training and self.use_residual:\n",
    "            if torch.rand(1)[0] < self.p:\n",
    "                return x\n",
    "        res = x\n",
    "        \n",
    "        if self.expand != 1:                                        ## 확장 비율이 1인경우는 확장이 되지 않는 것이기 때문에 생략함\n",
    "            x = self.expasion(x)\n",
    "            \n",
    "        x = self.depthwise(x)\n",
    "        x = self.seblock(x)\n",
    "        x = self.pointwise(x)\n",
    "        \n",
    "        if self.training and (self.dropout_rate is not None):       ## self.training은 훈련중인지를 판단 -> 훈련 중이면 드롭아웃 실시\n",
    "                x = F.dropout2d(input=x, p=self.dropout_rate, training=self.training, inplace=True)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            x = x + res\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Chaeck용\n",
    "#if __name__ == '__main__':\n",
    "#    x = torch.randn(3, 16, 17, 17)\n",
    "#    model = MBConv(16,24,3,6)\n",
    "#    output = model(x)\n",
    "#    print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_coef=1., depth_coef=1., resolution_coef=1., dropout=0.2):\n",
    "        super().__init__()\n",
    "        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
    "        repeats = [1, 2, 2, 3, 3, 4, 1]\n",
    "        strides = [1, 2, 2, 2, 1, 2, 1]\n",
    "        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n",
    "        depth = depth_coef\n",
    "        width = width_coef\n",
    "        depth_divisor = 8\n",
    "\n",
    "        repeats = [round_repeats(x, depth) for x in repeats]\n",
    "        self.p = []\n",
    "        for i in range(sum(repeats)):\n",
    "            self.p.append(i * 0.008695652173913044)\n",
    "        self.p_idx = 0\n",
    "        \n",
    "        #print(channels)\n",
    "        #print(repeats)\n",
    "        # efficient net\n",
    "\n",
    "        #self.upsample = lambda x: F.interpolate(x, scale_factor=resolution_coef, mode='bilinear', align_corners=False)      # 채널 수는 그대로 H x W의 크기를 resolution_coef 값을 곱한만큼으로 조정, \n",
    "        # upsample된 크기의 이미지를 바랄 뿐이고 내가 직접 건드릴 필요는 없다.(keras에서 그렇게 구현함)\n",
    "\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels[0], momentum=0.99),\n",
    "            nn.SiLU(inplace=True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.stage2 = self._make_Block(MBConv, repeats[0], channels[0], round_filters(channels[1],width,depth_divisor), kernel_size[0], strides[0], expand = 1, padding_size = 1)\n",
    "\n",
    "        self.stage3 = self._make_Block(MBConv, repeats[1], round_filters(channels[1],width,depth_divisor), round_filters(channels[2],width,depth_divisor), kernel_size[1], strides[1], expand = 6, padding_size = 1)\n",
    "\n",
    "        self.stage4 = self._make_Block(MBConv, repeats[2], round_filters(channels[2],width,depth_divisor), round_filters(channels[3],width,depth_divisor), kernel_size[2], strides[2], expand = 6, padding_size = 2)\n",
    "\n",
    "        self.stage5 = self._make_Block(MBConv, repeats[3], round_filters(channels[3],width,depth_divisor), round_filters(channels[4],width,depth_divisor), kernel_size[3], strides[3], expand = 6, padding_size = 1)\n",
    "\n",
    "        self.stage6 = self._make_Block(MBConv, repeats[4], round_filters(channels[4],width,depth_divisor), round_filters(channels[5],width,depth_divisor), kernel_size[4], strides[4], expand = 6, padding_size = 2)\n",
    "\n",
    "        self.stage7 = self._make_Block(MBConv, repeats[5], round_filters(channels[5],width,depth_divisor), round_filters(channels[6],width,depth_divisor), kernel_size[5], strides[5], expand = 6, padding_size = 2)\n",
    "\n",
    "        self.stage8 = self._make_Block(MBConv, repeats[6], round_filters(channels[6],width,depth_divisor), round_filters(channels[7],width,depth_divisor), kernel_size[6], strides[6], expand = 6, padding_size = 1)\n",
    "\n",
    "        self.stage9 = nn.Sequential(\n",
    "            nn.Conv2d(round_filters(channels[7],width,depth_divisor), round_filters(channels[8],width,depth_divisor), kernel_size = 1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(round_filters(channels[8],width,depth_divisor), momentum=0.99),\n",
    "            nn.SiLU()\n",
    "        ) \n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(round_filters(channels[8],width,depth_divisor), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f'input = {x.size()} ')\n",
    "        #x = self.upsample(x)\n",
    "        #print(f'upsample = {x.size()}')\n",
    "        x = self.stage1(x)\n",
    "        #print(f'stage 1 = {x.size()}')\n",
    "        x = self.stage2(x)\n",
    "        #print(f'stage 2 = {x.size()}')\n",
    "        x = self.stage3(x)\n",
    "        #print(f'stage 3 = {x.size()}')\n",
    "        x = self.stage4(x)\n",
    "        #print(f'stage 4 = {x.size()}')\n",
    "        x = self.stage5(x)\n",
    "        #print(f'stage 5 = {x.size()}')\n",
    "        x = self.stage6(x)\n",
    "        #print(f'stage 6 = {x.size()}')\n",
    "        x = self.stage7(x)\n",
    "        #print(f'stage 7 = {x.size()}')\n",
    "        x = self.stage8(x)\n",
    "        #print(f'stage 8 = {x.size()}')\n",
    "        x = self.stage9(x)\n",
    "        #print(f'stage 9 = {x.size()}')\n",
    "        x = self.avgpool(x)\n",
    "        #print(f'stage pool= {x.size()}')\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, expand, padding_size):\n",
    "        strides = [stride] + [1] * (repeats - 1)    #[stride, 1, 1, 1, 1] 이런 식으로 만들어줌\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(block(in_channels, out_channels, kernel_size, expand, stride = stride, padding_size = padding_size, p=self.p[self.p_idx]))\n",
    "            in_channels = out_channels\n",
    "            self.p_idx += 1\n",
    "            #print(layers)\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EfficientNetB0(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.0, depth_coef= 1.0, resolution_coef = 224/224, dropout=0.2)\n",
    "\n",
    "def EfficientNetB1(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.0, depth_coef= 1.1, resolution_coef = 240/224, dropout=0.2)\n",
    "\n",
    "def EfficientNetB2(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.1, depth_coef= 1.2, resolution_coef = 260/224, dropout=0.3)\n",
    "\n",
    "def EfficientNetB3(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.2, depth_coef= 1.4, resolution_coef = 300/224, dropout=0.3)\n",
    "\n",
    "def EfficientNetB4(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.4, depth_coef= 1.8, resolution_coef = 380/224, dropout=0.4)\n",
    "\n",
    "def EfficientNetB5(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.6, depth_coef= 2.2, resolution_coef = 456/224, dropout=0.4)\n",
    "\n",
    "def EfficientNetB6(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=1.8, depth_coef= 2.6, resolution_coef = 528/224, dropout=0.5)\n",
    "\n",
    "def EfficientNetB7(num_class = 1000):\n",
    "  return EfficientNet(num_classes = num_class, width_coef=2.0, depth_coef= 3.1, resolution_coef = 600/224, dropout=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
